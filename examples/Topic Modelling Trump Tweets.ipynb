{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\organis2\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "import pandas as pd\n",
    "from gensim.corpora.dictionary import Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Date</th>\n",
       "      <th>Favorites</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Tweet ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nielson Media Research final numbers on ACCEPT...</td>\n",
       "      <td>2016-07-30 23:32:40</td>\n",
       "      <td>13850</td>\n",
       "      <td>4130</td>\n",
       "      <td>759592590106849280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thank you to all of the television viewers tha...</td>\n",
       "      <td>2016-07-30 19:00:07</td>\n",
       "      <td>27659</td>\n",
       "      <td>6842</td>\n",
       "      <td>759524001613918208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you imagine if I had the small crowds that...</td>\n",
       "      <td>2016-07-30 18:28:22</td>\n",
       "      <td>19968</td>\n",
       "      <td>6488</td>\n",
       "      <td>759516008272932864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NATO commander agrees members should pay up vi...</td>\n",
       "      <td>2016-07-30 18:24:40</td>\n",
       "      <td>11624</td>\n",
       "      <td>4668</td>\n",
       "      <td>759515080010719232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow, NATO's top commander just announced that ...</td>\n",
       "      <td>2016-07-30 18:18:58</td>\n",
       "      <td>23922</td>\n",
       "      <td>7819</td>\n",
       "      <td>759513644258525184</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                 Date  \\\n",
       "0  Nielson Media Research final numbers on ACCEPT...  2016-07-30 23:32:40   \n",
       "1  Thank you to all of the television viewers tha...  2016-07-30 19:00:07   \n",
       "2  Can you imagine if I had the small crowds that...  2016-07-30 18:28:22   \n",
       "3  NATO commander agrees members should pay up vi...  2016-07-30 18:24:40   \n",
       "4  Wow, NATO's top commander just announced that ...  2016-07-30 18:18:58   \n",
       "\n",
       "   Favorites  Retweets            Tweet ID  \n",
       "0      13850      4130  759592590106849280  \n",
       "1      27659      6842  759524001613918208  \n",
       "2      19968      6488  759516008272932864  \n",
       "3      11624      4668  759515080010719232  \n",
       "4      23922      7819  759513644258525184  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv(\"https://raw.githubusercontent.com/sashaperigo/Trump-Tweets/master/data.csv\").dropna()\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      #a\n",
       "1    test\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize tweets, while stoplisting, case-folding, and filtering\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stoplist = stopwords.words('english')\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    lower = tweet.lower()\n",
    "    # Small hack to keep hashtags without modifying tokenizer:\n",
    "    #   replace # with text, then replace back later\n",
    "    terms = word_tokenize(lower.replace(\"#\", \"HASH_\"))\n",
    "    terms_stopped = [term for term in terms if term not in stoplist]\n",
    "    terms_alpha = [term for term in terms_stopped if (term.isalpha() or \"HASH_\" in term)]\n",
    "    if len(terms_alpha) == 0:\n",
    "        return pd.Series()\n",
    "    else:\n",
    "        return pd.Series(terms_alpha).str.replace(\"HASH_\", \"#\")\n",
    "    \n",
    "clean_tweet(\"This is #a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>word</th>\n",
       "      <th>#</th>\n",
       "      <th>#1</th>\n",
       "      <th>#2</th>\n",
       "      <th>#2016</th>\n",
       "      <th>#2a</th>\n",
       "      <th>#alsicebucketchallenge</th>\n",
       "      <th>#america</th>\n",
       "      <th>#americafirst</th>\n",
       "      <th>#apprentice</th>\n",
       "      <th>#autism</th>\n",
       "      <th>...</th>\n",
       "      <th>yrs</th>\n",
       "      <th>yuan</th>\n",
       "      <th>zero</th>\n",
       "      <th>zimmerman</th>\n",
       "      <th>zogby</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zuckerman</th>\n",
       "      <th>zuker</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tweet ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1698308935</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1701461182</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737479987</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741160716</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773561338</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5445 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "word          #   #1   #2  #2016  #2a  #alsicebucketchallenge  #america  \\\n",
       "Tweet ID                                                                  \n",
       "1698308935  0.0  0.0  0.0    0.0  0.0                     0.0       0.0   \n",
       "1701461182  0.0  0.0  0.0    0.0  0.0                     0.0       0.0   \n",
       "1737479987  0.0  0.0  0.0    0.0  0.0                     0.0       0.0   \n",
       "1741160716  0.0  0.0  0.0    0.0  0.0                     0.0       0.0   \n",
       "1773561338  0.0  0.0  0.0    0.0  0.0                     0.0       0.0   \n",
       "\n",
       "word        #americafirst  #apprentice  #autism  ...    yrs  yuan  zero  \\\n",
       "Tweet ID                                         ...                      \n",
       "1698308935            0.0          0.0      0.0  ...    0.0   0.0   0.0   \n",
       "1701461182            0.0          0.0      0.0  ...    0.0   0.0   0.0   \n",
       "1737479987            0.0          0.0      0.0  ...    0.0   0.0   0.0   \n",
       "1741160716            0.0          0.0      0.0  ...    0.0   0.0   0.0   \n",
       "1773561338            0.0          0.0      0.0  ...    0.0   0.0   0.0   \n",
       "\n",
       "word        zimmerman  zogby  zone  zones  zucker  zuckerman  zuker  \n",
       "Tweet ID                                                             \n",
       "1698308935        0.0    0.0   0.0    0.0     0.0        0.0    0.0  \n",
       "1701461182        0.0    0.0   0.0    0.0     0.0        0.0    0.0  \n",
       "1737479987        0.0    0.0   0.0    0.0     0.0        0.0    0.0  \n",
       "1741160716        0.0    0.0   0.0    0.0     0.0        0.0    0.0  \n",
       "1773561338        0.0    0.0   0.0    0.0     0.0        0.0    0.0  \n",
       "\n",
       "[5 rows x 5445 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 'long' dataframe of term counts\n",
    "tweet_words = tweets['Text'].str.lower().apply(clean_tweet)\n",
    "tweet_words.index = tweets['Tweet ID']\n",
    "\n",
    "word_counts = (tweet_words.stack().to_frame()\n",
    "                          .reset_index()\n",
    "                          .rename(columns={0:'word', 'level_1':'count'})\n",
    "                          .groupby(['Tweet ID', 'word'], as_index=False).count()\n",
    "              )\n",
    "\n",
    "# Filter to words that have been used 5 or more times\n",
    "words_filtered = word_counts.groupby('word').filter(lambda x: x['count'].sum() >= 5)\n",
    "\n",
    "# Make 'wide' dataframe, i.e. a document-term matrix\n",
    "trump_counts = words_filtered.pivot(index='Tweet ID', columns='word', values='count').fillna(0)\n",
    "trump_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of our document-term matrix, `count(tweets) x count(unique_words)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "901     \"@NathanDWilsonFL: @MariaBartiromo you had a g...\n",
       "2821    \"@AniesiODaniels: #DemDebate Q: Who are you vo...\n",
       "3646    \"@TradingStreetCo:Donald Trump Is Ratings ‘Gol...\n",
       "4359    \"@moshe_mkmdca: @realDonaldTrump @007lLisav @C...\n",
       "4981    \"@jimlibertarian:  @SlwStdySque Donald has alr...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = trump_counts.loc[:,[\"donald\"]].query('donald > 1').index.values\n",
    "tweets[tweets[\"Tweet ID\"].isin(q)]['Text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number all the columns and create a gensim dictionary\n",
    "dictionary = Dictionary()\n",
    "dictionary.token2id = dict(zip(trump_counts.columns, range(0, trump_counts.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If I haven't already trained and saved a model, train it now\n",
    "if not os.path.exists('trump-tweets.pickle'):\n",
    "    # Train a model\n",
    "    # Gensim has a way to read numpy arrays, but they use columns for documents - so rotate ('transpose') the DataFrame\n",
    "    corpus = gensim.matutils.Dense2Corpus(trump_counts.values.T)\n",
    "    lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary,\n",
    "                                          num_topics=20, update_every=1, chunksize=1000, passes=6, alpha='auto')\n",
    "    lda.save('trump-tweets.pickle')\n",
    "else:\n",
    "    # Load a model\n",
    "    lda = gensim.models.ldamodel.LdaModel.load('trump-tweets.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0.060*\"nice\" + 0.054*\"got\" + 0.052*\"wow\" + 0.050*\"say\" + 0.038*\"nothing\" + 0.032*\"wonderful\"\n",
      "1\t0.070*\"cnn\" + 0.068*\"poll\" + 0.046*\"think\" + 0.041*\"true\" + 0.037*\"day\" + 0.036*\"man\"\n",
      "2\t0.279*\"thank\" + 0.090*\"vote\" + 0.087*\"big\" + 0.036*\"crowd\" + 0.035*\"needs\" + 0.018*\"apprentice\"\n",
      "3\t0.092*\"clinton\" + 0.043*\"megynkelly\" + 0.042*\"ever\" + 0.029*\"presidential\" + 0.024*\"women\" + 0.021*\"truth\"\n",
      "4\t0.098*\"people\" + 0.072*\"get\" + 0.048*\"cruz\" + 0.047*\"many\" + 0.040*\"bad\" + 0.036*\"really\"\n",
      "5\t0.181*\"http\" + 0.139*\"trump\" + 0.103*\"donald\" + 0.045*\"via\" + 0.022*\"morning\" + 0.020*\"hampshire\"\n",
      "6\t0.090*\"make\" + 0.087*\"foxnews\" + 0.063*\"win\" + 0.040*\"gop\" + 0.039*\"interview\" + 0.038*\"foxandfriends\"\n",
      "7\t0.043*\"hope\" + 0.038*\"watching\" + 0.032*\"person\" + 0.031*\"far\" + 0.028*\"year\" + 0.027*\"party\"\n",
      "8\t0.070*\"see\" + 0.050*\"know\" + 0.047*\"tomorrow\" + 0.045*\"speech\" + 0.037*\"let\" + 0.037*\"years\"\n",
      "9\t0.119*\"#makeamericagreatagain\" + 0.043*\"support\" + 0.040*\"campaign\" + 0.040*\"jobs\" + 0.035*\"american\" + 0.034*\"join\"\n",
      "10\t0.201*\"great\" + 0.044*\"thanks\" + 0.038*\"tonight\" + 0.031*\"today\" + 0.030*\"show\" + 0.029*\"last\"\n",
      "11\t0.099*\"president\" + 0.071*\"would\" + 0.049*\"good\" + 0.043*\"obama\" + 0.042*\"never\" + 0.042*\"need\"\n",
      "12\t0.043*\"work\" + 0.037*\"national\" + 0.032*\"oreillyfactor\" + 0.029*\"golf\" + 0.027*\"hard\" + 0.027*\"place\"\n",
      "13\t0.220*\"realdonaldtrump\" + 0.076*\"trump\" + 0.052*\"hillary\" + 0.051*\"america\" + 0.047*\"#trump2016\" + 0.032*\"like\"\n",
      "14\t0.180*\"https\" + 0.123*\"new\" + 0.079*\"crooked\" + 0.022*\"york\" + 0.020*\"rally\" + 0.019*\"politicians\"\n",
      "15\t0.063*\"much\" + 0.039*\"republican\" + 0.038*\"better\" + 0.037*\"money\" + 0.037*\"bernie\" + 0.033*\"deal\"\n",
      "16\t0.038*\"happy\" + 0.038*\"jebbush\" + 0.036*\"change\" + 0.032*\"florida\" + 0.026*\"endorsement\" + 0.025*\"ready\"\n",
      "17\t0.046*\"keep\" + 0.037*\"donaldtrump\" + 0.035*\"soon\" + 0.033*\"wants\" + 0.026*\"agree\" + 0.026*\"sanders\"\n",
      "18\t0.094*\"love\" + 0.058*\"ted\" + 0.035*\"isis\" + 0.026*\"immigration\" + 0.021*\"terrible\" + 0.020*\"wo\"\n",
      "19\t0.083*\"country\" + 0.042*\"even\" + 0.039*\"right\" + 0.038*\"debate\" + 0.037*\"media\" + 0.035*\"must\"\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([\"%d\\t%s\" % info for info in lda.show_topics(num_topics=20, num_words=6)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5445)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.state.get_lambda().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-5c0fdef1a4e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_document_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\organis2\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mget_document_topics\u001b[1;34m(self, bow, minimum_probability, minimum_phi_value, per_word_topics)\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m         \u001b[0mtopic_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# normalize distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\organis2\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[1;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteger_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m                 \u001b[1;31m# make sure the term IDs are ints, otherwise np will get upset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m                 \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\organis2\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteger_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m                 \u001b[1;31m# make sure the term IDs are ints, otherwise np will get upset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m                 \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[0mids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mid\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "a = lda.get_document_topics(corpus.dense.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
